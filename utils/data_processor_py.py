# -*- coding: utf-8 -*-
"""data_processor.py

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hriuAMnDcxqnyHboyne-mUI2sz5XO32H
"""

# Calculate Total Revenue
def calculate_total_revenue(csv_path: str) -> float:
    df = pd.read_csv(csv_path)
    df["UnitPrice"] = (
        df["UnitPrice"]
        .astype(str)
        .str.replace(",", "", regex=False)
        .astype(float)
    )
    df["Revenue"] = df["Quantity"] * df["UnitPrice"]
    return float(df["Revenue"].sum())

# Call the function and print the result
csv_file = "sales_data_.csv"   # path to your file
total_rev = calculate_total_revenue(csv_file)
print("Total revenue:", total_rev)

# Region wise sales analysis
df = pd.read_csv("sales_data_.csv")

# Clean UnitPrice (remove commas) and convert to numeric
df["UnitPrice"] = (
    df["UnitPrice"]
    .astype(str)
    .str.replace(",", "", regex=False)
    .astype(float)
)

# Calculate Sales
df["Sales"] = df["Quantity"] * df["UnitPrice"]

# Region-wise aggregation
region_summary = (
    df.groupby("Region", dropna=False)
      .agg(
          total_sales=("Sales", "sum"),
          transaction_count=("TransactionID", "count")
      )
      .reset_index()
)

# Percentage of total sales
total_sales_all = region_summary["total_sales"].sum()
region_summary["sales_pct"] = (
    region_summary["total_sales"] / total_sales_all * 100
)

# Sort by total_sales in descending order
region_summary = region_summary.sort_values(
    "total_sales", ascending=False
)

print(region_summary)

# Top Selling Products
def top_selling_products(transactions: pd.DataFrame, n: int = 5):

    # Ensure UnitPrice is numeric (handles commas as thousands separators)
    transactions = transactions.copy()
    transactions["UnitPrice"] = (
        transactions["UnitPrice"]
        .astype(str)
        .str.replace(",", "", regex=False)
        .astype(float)
    )

    # Calculate revenue per transaction
    transactions["Revenue"] = transactions["Quantity"] * transactions["UnitPrice"]

    # Aggregate by ProductName
    product_stats = (
        transactions
        .groupby("ProductName", as_index=False)
        .agg(
            TotalQuantity=("Quantity", "sum"),
            TotalRevenue=("Revenue", "sum")
        )
    )

    # Sort by TotalQuantity descending
    product_stats = product_stats.sort_values(
        "TotalQuantity", ascending=False
    )

    # Take top n and convert to list of tuples
    top_n = [
        (row["ProductName"], int(row["TotalQuantity"]), float(row["TotalRevenue"]))
        for _, row in product_stats.head(n).iterrows()
    ]

    return top_n

# Example usage:
# df = pd.read_csv("sales_data_.csv")
print(top_selling_products(df, n=5))

# Customer Analysis (transactions)
def customer_analysis(transactions: pd.DataFrame):

    df = transactions.copy()

    # Clean UnitPrice (handles values like "1,916")
    df["UnitPrice"] = (
        df["UnitPrice"]
        .astype(str)
        .str.replace(",", "", regex=False)
        .astype(float)
    )

    # Calculate revenue per transaction
    df["Revenue"] = df["Quantity"] * df["UnitPrice"]

    # Aggregate per customer
    customer_stats = (
        df.groupby("CustomerID")
          .agg(
              total_spent=("Revenue", "sum"),
              purchase_count=("TransactionID", "count"),
          )
          .reset_index()
    )

    # Average order value
    customer_stats["avg_order_value"] = (
        customer_stats["total_spent"] / customer_stats["purchase_count"]
    )

    # Unique products bought per customer
    products_per_customer = (
        df.groupby("CustomerID")["ProductName"]
          .apply(lambda x: sorted(x.unique().tolist()))
          .reset_index(name="products_bought")
    )

    # Merge stats and products
    merged = customer_stats.merge(products_per_customer, on="CustomerID")

    # Sort by total_spent descending
    merged = merged.sort_values("total_spent", ascending=False)

    # Convert to expected dictionary format
    result = {}
    for _, row in merged.iterrows():
        cid = row["CustomerID"]
        result[cid] = {
            "total_spent": float(row["total_spent"]),
            "purchase_count": int(row["purchase_count"]),
            "avg_order_value": float(row["avg_order_value"]),
            "products_bought": row["products_bought"],
        }

    return result

# Example usage:
# df = pd.read_csv("sales_data.csv")
stats = customer_analysis(df)
print(stats)

import pandas as pd

# Daily Sales Trend

def daily_sales_trend(transactions: pd.DataFrame):

    df = transactions.copy()

    # Parse Date column to datetime, then normalize to date only
    df["Date"] = pd.to_datetime(df["Date"], format="%m/%d/%Y")
    df["Date"] = df["Date"].dt.date  # keep only date part

    # Clean UnitPrice (handles values like "1,916")
    df["UnitPrice"] = (
        df["UnitPrice"]
        .astype(str)
        .str.replace(",", "", regex=False)
        .astype(float)
    )

    # Calculate revenue per transaction
    df["Revenue"] = df["Quantity"] * df["UnitPrice"]

    # Group by Date
    daily = (
        df.groupby("Date")
          .agg(
              revenue=("Revenue", "sum"),
              transaction_count=("TransactionID", "count"),
              unique_customers=("CustomerID", "nunique"),
          )
          .reset_index()
    )

    # Sort chronologically
    daily = daily.sort_values("Date")

    # Build result dictionary with ISO date strings as keys
    result = {}
    for _, row in daily.iterrows():
        date_str = row["Date"].isoformat()
        result[date_str] = {
            "revenue": float(row["revenue"]),
            "transaction_count": int(row["transaction_count"]),
            "unique_customers": int(row["unique_customers"]),
        }

    return result

# Example usage:
df = pd.read_csv("sales_data_.csv")
trend = daily_sales_trend(df)
print(trend)

# Peak sales dates
import pandas as pd

def find_peak_sales_day(transactions: pd.DataFrame):
    """
    Identifies the date with highest revenue.

    Returns:
        tuple: (date_str, revenue, transaction_count)
        Example: ('2024-12-15', 185000.0, 12)
    """

    df = transactions.copy()

    # Parse Date column to datetime (format in your file is MM/DD/YYYY)
    df["Date"] = pd.to_datetime(df["Date"], format="%m/%d/%Y")  # [file:1]

    # Clean UnitPrice (handles values like "1,916") and convert to float
    df["UnitPrice"] = (
        df["UnitPrice"]
        .astype(str)
        .str.replace(",", "", regex=False)
        .astype(float)
    )  # [file:1]

    # Calculate revenue per transaction
    df["Revenue"] = df["Quantity"] * df["UnitPrice"]  # [file:1]

    # Aggregate by date
    daily = (
        df.groupby("Date")
          .agg(
              revenue=("Revenue", "sum"),
              transaction_count=("TransactionID", "count"),
          )
          .reset_index()
    )  # [file:1]

    # Find row with maximum revenue
    peak_row = daily.loc[daily["revenue"].idxmax()]  # [file:1]

    # Convert date to ISO string
    date_str = peak_row["Date"].date().isoformat()

    return (
        date_str,
        float(peak_row["revenue"]),
        int(peak_row["transaction_count"]),
    )

# Example usage:
df = pd.read_csv("sales_data_.csv")
peak_day = find_peak_sales_day(df)
print(peak_day)

# Low Performing Products
import pandas as pd

def low_performing_products(transactions: pd.DataFrame, threshold: int = 10):

    df = transactions.copy()

    # Clean UnitPrice (handles values like "1,692")
    df["UnitPrice"] = (
        df["UnitPrice"]
        .astype(str)
        .str.replace(",", "", regex=False)
        .astype(float)
    )  # [file:1]

    # Revenue per transaction
    df["Revenue"] = df["Quantity"] * df["UnitPrice"]  # [file:1]

    # Aggregate by ProductName
    product_stats = (
        df.groupby("ProductName", as_index=False)
          .agg(
              TotalQuantity=("Quantity", "sum"),
              TotalRevenue=("Revenue", "sum"),
          )
    )  # [file:1]

    # Filter products with total quantity < threshold
    low_perf = product_stats[product_stats["TotalQuantity"] < threshold]  # [file:1]

    # Sort by TotalQuantity ascending
    low_perf = low_perf.sort_values("TotalQuantity", ascending=True)  # [file:1]

    # Convert to list of tuples
    result = [
        (row["ProductName"], int(row["TotalQuantity"]), float(row["TotalRevenue"]))
        for _, row in low_perf.iterrows()
    ]  # [file:1]

    return result

# Example usage:
df = pd.read_csv("sales_data_.csv")
print(low_performing_products(df, threshold=10))